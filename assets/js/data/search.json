[ { "title": "[부스트캠프 AI] Week3 회고", "url": "/posts/Week3-%ED%9A%8C%EA%B3%A0/", "categories": "Boostcamp AI", "tags": "Boostcamp AI", "date": "2022-02-04 00:00:00 +0900", "snippet": "부스트캠프 3주차 회고💡 느낀 점  연휴가 있어 목, 금의 학습 분량이 있었다. 학습 피로도면에서는 덜했지만 운용할 수 있는 시간이 적은만큼 더 정신없이 지나갔던 듯하다.💡 좋았던 점  개인적으로 시각화는 필요할 때, 공부를 할 때 가장 어려움을 겪었던 부분이었다. 다른 분야에 비해 정보들이 단편적으로 있고, 여러가지 방법으로 활용가능하지만 모두의 스타일이 달랐기 때문이다. 이번주 강의를 듣고 순서대로, 너무 깔끔하게, 정리를 해주셔서 그동안 헤맸던 부분들이 정리가 되어 좋았다.  마스터세션 또한 너무 유익했다. 마스터님께서 많은 고민과 노력을 이루는 분이라는 것이 느껴졌고, 긍정적인 자극을 많이 받았다.💡 아쉬웠던 점  그래프의 종류나 요소들이 많은데, 이를 다 활용할 수 있다!라고는 말할 수 없다. 다른 예시로, 다양한 방법으로 많이 그려봐야 잘 활용할 수 있을 것이라고 생각된다.   깃 활용을 위해 피어세션에서 서로의 브랜치를 만들고 코드를 올려봤는데, 공유하는 레포도 처음이고 브랜치도 처음이라 많이 버벅거려서 아쉬웠다. 많이 활용해보고 더 공부해야지..💡 정리  설 연휴에 최소한으로 목표했던 일들을 했음에 감사했고, 개인적으로 고민했던 것들을 조금 정리할 수 있는 한 주였다. 아직도 할 게 너무 많지만, 막연히 목표했던 것들이 조금씩 쌓여가고 있다는 것을 최근에 많이 느끼고 있다. 이를 원동력으로 계속 나아가자🔥" }, { "title": "[부스트캠프 AI] Day12", "url": "/posts/Day12/", "categories": "Boostcamp AI, Python, DataViz", "tags": "Boostcamp AI", "date": "2022-02-04 00:00:00 +0900", "snippet": "[DataViz]Text 사용하기Matplotlib에서 Text Text in Viz : Visual representation들이 줄 수 없는 많은 설명 추가, 오해 방지 가능 Anatomy of a Figure Title, Label, Tick Label, Annotation(화살표로 포인팅 가능), Text Text 사용법 .set_xlabel(), .set_ylabel(), set_title(), .legend(title=’’) annotate(text=&#39;&#39;) → 화살표 추가 bbox - 박스 스타일, 색 등 지정 Color 사용하기Color에 대한 이해 위치(시각화 방법에 따라 결정)와 색(직접 선택)은 가장 효과적인 채널 구분 독자에게 원하는 인사이트 전달이 중요 → 화려함이 전부 X, 오용 방지 통용되는 색 → 다른 사례 스터디 → 원하는 색Color Palette의 종류 Categorical - 최대 10개 색상까지 사용, 색의 차이로 구분 Sequential - 연속적 색상을 사용, 단일 색조로 표현 → 2차원 데이터에서 많이 사용 / 지리지도 데이터, 계층형 데이터에도 적합 → imshow Diverge(발산형) - 중앙(양쪽 점에서 편향되지 않는 색)을 기준으로 발산, 양 끝으로 갈수록 진해짐. → matplotlib.colors.TwoSlopNorm 그 외 색 Tips 강조 - Highlighting, 색상 대비 사용 명도 대비 / 색상 대비 / 채도 대비 / 보색 대비 *HSI → Hue(색조), Saturate(채도), Lightness(광도) Facet 사용하기Multiple View Facet : 분할을 의미, view를 분할 및 추가하여 다양한 관점 전달Matplotlib에서 구현 Figure(큰 틀, 늘 1개), Axes(각 플롯이 들어가는 공간, N개) NxM subplots plt.subplot(), plt.figure()+fig.add_subplot(), plot.subplots() → figuresize / dpi(해상도) / sharex, sharey(축 공유) / squeeze / aspect Grid Spec의 활용 section 구분된 Subplot Slicing 사용(numpy) fig.add_grid_spec() x, y, dx, dy 사용 fig.subplot2grid() 내부에 그리기 - ax 내부에 서브플롯 추가 ax.inset_axes() → bar plot 안에 pie chart로 비중 함께 표시 → 사이드에 추가 - colorbar에 많이 사용 make_axes_locatable(ax) More Tips 정보 늘리고 다채롭게 표현하기Grid 이해하기 Default Grid 무채색(color) 맨 밑에 오도록 조정 ax.grid(*zorder=0*) 큰 격자/세부 격자 ax.grid(*which = ‘major’, ‘minor’, ‘both’*) 다양한 타임의 Grid → x+y=c(합 중요), y=cx(비율 중요), xy=c, 동심원(거리)심플한 처리 선 추가하기 - 상한/하한선 면 추가 - 가독성 증가Setting 바꾸기 mpl.rc() or plt.rc() Theme mpl.style.use(&#39;&#39;) → fivethirtyeight, ggplot, seaborn, .. Line &amp;amp; Span - 강조" }, { "title": "[부스트캠프 AI] Day11", "url": "/posts/Day11/", "categories": "Boostcamp AI, Python, DataViz", "tags": "Boostcamp AI", "date": "2022-02-03 00:00:00 +0900", "snippet": "[DataViz]Welcome to Visualization데이터 시각화 : 데이터를 그래픽 요소로 매핑하여 시각적으로 표현하는 것→ 목적, 독자, 데이터, 스토리, 방법, 디자인시각화의 요소‘데이터’ 시각화 시각화를 진행할 데이터 : 데이터셋 관점(global), 개별 데이터 관점(local) 종류 정형 데이터 : item, attribute(feature) / 관계, 비교 시계열 데이터 : 추세(Trend), 계절성(Seasonality), 주기성(Cycle) 등을 살핌 지리 데이터 : 지도 정보와 보고자 하는 정보간의 조화 중요 + 단순화 시키는 경우도 존재 관계형(네트워크) 데이터 : 객체 - Node, 관계 - Link / 객체간의 관계를 시각화 계층적 데이터 : 관계 중에서도 포함관계가 분명한 데이터 - Tree, Treemap, Sunburst .. 다양한 비정형 데이터 다양하게 분류 가능 : 수치형[연속형, 이산형], 범주형[명목형, 순서형] 시각화의 이해 Mark &amp;amp; Channel Mark : 점, 선, 면으로 이루어진 데이터 시각화 Channel : 각 마크를 변경할 수 있는 요소들 전주의적 속성 주의를 주지 않아도 인지하게 되는 요소 동시에 사용하면 인지 어려움 → 적절 : 시각적 분리(visual popout) Bar PlotBar Plot? 범주(category)에 따른 수치 값 비교에 적합 (개별, 그룹 모두 적합) 분류 방향에 따른 분류 : 수직(default) - .bar(), 수평 - .barh() 다양한 Bar Plot 플롯을 여러개 그리기 한 개의 플롯에 동시에 나타내기 쌓아서 표현(Stacked Bar Plot) 맨 밑의 bar 분포는 파악하기 쉽지만 그 외의 분포는 파악 어려움 겹쳐서 표현(Overlapped Bar Plot) 2개 그룹만 비교할 경우 (3개 이상에서는 파악 어려움) 같은 축을 사용하여 비교 쉬움 - 투명도 조절 *alpha* 이웃에 배치하여 표현(Grouped Bar Plot) Matplotlib로는 구현 까다로움 (seaborn 사용) 그룹 5~7개일 때 효과적 - 많은 경우 ETC로 처리 정확한 Bar Plot Principle of Proportion Ink 실제 값과 그에 표현되는 그래픽의 잉크양 비례해야 함 x축의 시작은 zero 데이터 정렬 정확한 정보 전달을 위해 정렬이 필수적 sort_values(), sort_index() 종류에 따른 기준 시계열 - 시간순 수치형 - 크기순 순서형 - 범주 순서 명목형 - 범주 값 따라 대시보드에서는 Interactive로 제공하는 것이 유용 추가 고려요소 적절한 공간활용 - 여백과 공간 조정 복잡함과 단순함 - 시각화를 보는 대상이 누구인지 고려(EDA, Dashboard)Line PlotLine Plot?시간/순서에 대한 변화에 적합, 추세 살피기 위해 사용 .plot()Line Plot의 요소 색상(color), 마커(marker, markersize), 선의 종류(linestyle, linewidth) smoothing 사용 - Noise 인지적 방해 감소 가능 (주가 등 자세하게 봐야할 때는 smoothing X)정확한 Line Plot 추세에 집중 (x축 0에 초점 맞지 않아도 됨) 규칙적인 간격 데이터 아닌 경우 각 관측 값 점으로 표시 보간 : 데이터에 error나 noise 포함된 경우 이해를 도움 (오해 발생 가능 → 일반적인 분석에서는 지양) 이중 축 사용(지양) - .twinx(), 한 데이터 다른 단위 - .secondary_xaxis(), .secondary_yaxis() 범례 대신 라인 끝 단에 레이블 추가, Min/Max → 식별에 도움 (text 활용)Scatter Plot기본 Scatter Plot Scatter Plot? 두 feature간 관계를 알기 위해 사용 .scatter() Scatter Plot의 요소 색 (color), 모양(marker), 크기(size) Scatter Plot의 목적 상관관계 확인 / 군집 / 값 사이의 차이 / 이상치 정확한 Scatter Plot Overplotting : 점이 많을수록 분포 파악 어려움 → 투명도, jittering, 2차원 히스토그램, 등고선 사용 점의 요소와 인지 - 색, 마커, 크기 인과관계 ≠ 상관관계 추세선 - 패턴 유추 가능 Grid 사용 X" }, { "title": "[부스트캠프 AI] Week2 회고", "url": "/posts/Week2-%ED%9A%8C%EA%B3%A0/", "categories": "Boostcamp AI", "tags": "Boostcamp AI", "date": "2022-01-28 00:00:00 +0900", "snippet": "부스트캠프 2주차 회고💡 느낀 점  이번에도 과제의 양이 많아 시간적인 여유가 없음은 분명했지만 이번주가 더 만족스러웠다. 첫 주는 프리코스와 겹치고 이론적인 부분이 많아서 그런지 강의의 절대적인 양이 너무 많았다… 그러다보니 내가 지식을 쌓아간다는 느낌보다는 task를 해치운다는 느낌이 강했었다.  학습에 있어서는 메타인지가 가장 효과적+효율적이라고 생각하는데, 그저 강의를 듣는 것보다는 직접 해봐야 메타인지가 가능하다. 금주의 과정은 과제를 해결함으로써 성취감도 있었고, 내가 어떤 부분을 쉽게 해결하고, 어떤 부분에 대한 이해가 어려운지를 판단할 수 있는 기준이 되었어서 좋았다. (과제 해결보다는 제작에 더 많은 시간과 노력이 들었을거라고 생각해서 디테일하게 과제를 준비해주셔서 너무 감사했다 ㅠㅠ 🦆)💡 좋았던 점  피어세션을 통해서 함께 공부할 수 있다는 점이다.22 지난주에 비해 더욱 친해지고, 피어세션을 밀도있게 시간을 채웠다.  지난주 팀회고 시간에 다들 피어세션을 밀도있게 활용하고 싶다는 의견이 모아져서 금주부터 순서를 정해 발표를 시작했다. 모두에게 도움되는 자료를 준비해야 한다는 점이 조금 부담스럽게 다가왔었지만, 다들 열정적이고 경청하는 편이라 준비하는 사람도, 듣는 사람도 모두 유익한 시간으로 채워질 수 있었다. 과제도 어려워하는 부분은 이끌어주고, 풀이 방법을 공유하는 방식도 유익했다.  개인적인 학습의 측면에서는 PyTorch를 다뤄보기는 했지만 거의 짜여져 있는 모델을 가져와서 사용한 경우가 대부분이었기 때문에 부덕이🦆와 함께 모듈을 커스텀하고, 구현해보는 경험이 너무 좋았다.💡 아쉬웠던 점  강의와 과제에서 유익한 내용이 너무 많지만, 저작권 문제로 컨텐츠를 정리하는데 제약이 있다는 점이 당연한거지만 조금 아쉽다. 학습한 내용을 키워드로 정리하는 것. 배운 내용에 대한 설명을 정리하는 것 . 추가학습한 자료들을 정리하는 것  3가지 방식으로 분류할 수 있다면, 2번은 내용 유출을 신경써야하고, 3번을 하기에는 시간적 여유가 충분하지 않다는 딜레마가 있다. 스페셜 피어세션 때 이 고민을 공유 했었는데, 같은 고민을 했던 분이 있어 동질감이 들었다. 이 부분은 계속 고민해야할듯하다.💡 정리  설 연휴를 이용해서 공부 해야지! 하며 미뤄둔 일들이 많은데, 연휴가 의미없지 않도록 부족한 부분을 채우는 것도, 쉬어가는 시간도 확보할 수 있도록 우선순위와 시급성을 기반으로 목표를 설정하고, 시간을 어떻게 관리할 것인지를 미리 정하자." }, { "title": "[부스트캠프 AI] Day9, 10", "url": "/posts/Day9,-10/", "categories": "Boostcamp AI, Python, PyTorch", "tags": "Boostcamp AI", "date": "2022-01-28 00:00:00 +0900", "snippet": "[PyTorch]Multi-GPU 학습Model parallel 다중 GPU에 학습을 분산하는 두 가지 방법 모델 나누기 - 이전부터 사용(alexnet) 모델의 병목, 파이프라인의 어려움 → 모델 병렬화는 고난이도 과제 파이프라인 구축 - 순차적으로가 아닌 병렬적으로 수행되도록 구축 필요Data parallel 데이터를 나눠 GPU에 할당 후 결과의 평균을 취하는 방법 minibatch 수식과 유사, 한번에 여러 GPU에서 수행 in PyTorch DataParallel - 단순히 데이터를 분배한 후 평균을 취함 → GPU 사용 불균형 문제 발생, Batch size 감소, GIL DistributedDataParallel - 각 CPU마다 process 생성하여 개별 GPU에 할당 → 기본적으로 DataParallel로 하나 개별적으로 연산의 평균을 냄 💡 pin_memory : DRAM 메모리에 데이터를 바로바로 올릴 수 있도록 절차를 간소하게 데이터를 저장하는 방식 DataLoader에서 True로 바꾸면 Tensor를 CUDA 고정 메모리에 할당 고정된 메모리에서 데이터를 가져오기 때문에 데이터 전송이 훨씬 빨라짐 Hyperparameter Tuning모델 스스로 학습하지 않는 값은 사람이 지정 (learning rate, Model size, optimizer, ..) 모델의 성능 개선 방법 모델 바꾸기 (이익이 그리 크지 않음-유명한 모델들이 있기 때문) 데이터+ Hyperparameter Tuning 방법 grid search : 일정한 방법으로 자름 (로그를 취해서 값을 올려주는 경우 많음) random search 최근에는 베이지안 기법들이 주도 (BOHB, 2018) Ray (도구) - TensorboardX 사용 multi-node multi processing 지원 모듈 ML/DL 병렬 처리를 위해 개발된 모듈로, 현재의 분산병렬 ML/DL 모듈의 표준 Hyperparameter Search를 위한 다양한 모듈 제공 PyTorchtorchvision.transforms Resize() RandomCrop(), CenterCrop() RandomRotation() RandomHorizontalFlip(), RandomVerticalFlip() library - albumentationsEncoder &amp;amp; Decoder encoder(인코더) : 입력 데이터를 인코딩(부호화) - 입력 처리 decoder(디코더) : 인코딩된 데이터를 디코딩(복호화) - 결과 생성## make vocab dicttokenizer = torchtext.data.utils.get_tokenizer(&#39;basic_english&#39;)counter = collections.Counter()for line in lines: token = tokenizer(line) counter.update(token)vocab = torchtext.vocab.vocab(counter, min_freq=1)# encoder1encoder = vocab.get_stoi()# encoder2def encode(x): return [vocab.get_stoi()[s] for n, s in enumerate(tokenizer(x))]# decoder1decoder = vocab.get_itos()# decoder2def decode(x): return [vocab.get_itos()[i] for n, i in enumerate(x)]PyTorch TroubleshootingOOM이 해결 어려운 이유 발생 이유&amp;amp;원인 파악의 어려움 메모리 이전 상황의 파악이 어려움 해결 시도 : Batch size ↓ → GPU clean → RunGPUUtil nvidia-smi처럼 GPU의 상태를 보여주는 모듈 iter마다 메모리가 늘어나는지 확인!!torch.cuda.empty_cache() 사용되지 않은 GPU상 cache를 정리 가용 메모리 확보loop python 특성상 loop가 끝나도 메모리 공간 차지 training loop tensor로 축적 되는 변수 확인 필요 1-d tensor의 경우 python 기본 객체로 변환하여 처리 필요. del로 필요 없어진 변수 삭제" }, { "title": "[부스트캠프 AI] Day8", "url": "/posts/Day8/", "categories": "Boostcamp AI, Python, PyTorch", "tags": "Boostcamp AI", "date": "2022-01-26 00:00:00 +0900", "snippet": "[PyTorch]모델 불러오기model.save() 모델 형태(architecture)와 파라미터 저장 모델 학습 중간 과정의 저장을 통해 최선의 결과 모델 선택 state_dict() : 모델의 파라미터 표시# https://tutorials.pytorch.kr/beginner/saving_loading_models.html## 모델의 파라미터 저장하여 파라미터 load# 모델과 파라미터 저장 for params in model.state_dict(): print(params, &quot;\\t&quot;, model.state_dict()[params].size()) torch.save(model.state_dict(), PATH+&#39;model.pt&#39;) # ordered dict 형태# 같은 모델 형태에서 파라미터만 load 하기n_model = TheModelClass()n_model.load_state_dict(torch.load(PATH+&#39;model.pt&#39;))## 모델의 architecture와 함께 저장 &amp;amp; loadtorch.save(model, PATH+&#39;model.pt&#39;)model = torch.load(PATH+&#39;model.pt&#39;)checkpoints 학습의 중간 결과를 저장 → 최선의 결과 선택 earlystopping 사용시 이전 학습 결과물을 저장 loss, metric 값을 지속적으로 확인 저장 (epoch, loss, metric)# https://tutorials.pytorch.kr/beginner/saving_loading_models.htmltorch.save({&#39;epoch&#39; : epoch, &#39;model_state_dict&#39; : model.state_dict(), &#39;optimizer_state_dict&#39; : optimizer.sate_dict(), &#39;loss&#39; : loss}, &quot;file_name_.pt&quot;)checkpoint = torch.load(PATH)model.load_state_dict(checkpoint[&#39;model_state_dict&#39;])optimizer.load_state_dict(checkpoint[&#39;optimizer_state_dict&#39;])epoch = checkpoint[&#39;epoch&#39;]loss = checkpoint[&#39;loss&#39;]pretrained model Transfer learning다른 데이터셋으로 만든 모델을 현재 데이터에 적용 대용량 데이터셋으로 학습한 모델의 성능이 좋음 현재의 DL에서는 가장 일반적인 학습 기법 backbone architecture가 잘 학습된 모델에서 일부분만 변경하여 학습 수행 모델의 일부분을 frozen 시킴 모델을 가져와서 쓰되, 특정 위치까지만 쓰고 이후부터는 새로 학습시킴 → 기준 이전 : 파라미터 변경 X, 기준 이후 : 파라미터 업데이트 #https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html## vgg16 모델을 가져오기model = models.vgg16(pretrained=True)## vgg19 모델 마지막 레이어 제외하고 frozenclass Mymodel(nn.Module): def __init__(self): ... self.vgg19 = models.vgg19(pretrained=True) self.linear_layers = nn.Linear(input, output) def forward(self, x): x = self.vgg19(x) return self.linear_layers(x) # vgg 이후 마지막for param in my_model.parameters(): param.requires_grad = False # 마지막 레이어 제외 Frozenfor param in my_model.linear_layers.parameters(): param.requires_grad = True # 마지막 레이어는 학습(업데이트)Monitoring tools for PyTorchTensorboardTensorFlow의 프로젝트로 만들어진 시각화 도구 → PyTorch도 연결 가능 학습 그래프, metric, 학습 결과의 시각화를 지원 DL 시각화 구성 scalar : 상수 값의 연속(epoch) graph : 모델의 computational graph histogram : weight 등 값의 분포 Image : 예측-실제 값 비교 표시 mesh : 3d 형태의 데이터 표현 weight &amp;amp; biases(wandb)머신러닝 실험 원활히 지원 위한 도구 협업, code versioning, 실험 결과 기록 등 제공 MLOpsDataset &amp;amp; DataLoaderclass Dataset(Dataset)class DatasetA(Dataset): def __init__(self): A = load_A() self.X = torch.tensor(A.data) self.y = torch.tensor(A.target) self.feature_names = A.feature_names def __len__(self): return len(self.X) def __getitem__(self, idx): return torch.tensor(self.X[idx]), torch.tensor(self.y[idx])collate_fn하나의 batch에서 가장 길이가 긴 sample 기준으로 길이 맞추기def my_collate_fn(samples): inputs = [sample[&#39;X&#39;] for sample in samples] outputs = [sample[&#39;y&#39;] for sample in samples] coll_in = torch.nn.utils.rnn.pad_sequence(collate_X, batch_first=True) return {&#39;X&#39;: torch.stack(list(coll_in)), # 또는 coll_in.couguous(), &#39;y&#39;: torch.stack(collate_Y)}" }, { "title": "[부스트캠프 AI] Day7", "url": "/posts/Day7/", "categories": "Boostcamp AI, Python, PyTorch", "tags": "Boostcamp AI", "date": "2022-01-25 00:00:00 +0900", "snippet": "[PyTorch]PyTorch 프로젝트 구조 이해하기PyTorch Project Template OverviewJupyter [장점] - 개발 초기 단계에서는 대화식 개발 과정이 유리 (디버깅 과정) [단점] - 배포 및 공유 단계에서 공유 어려움 (재현 가능성의 한계) ⇒ DL 코드도 하나의 프로그램으로, 개발 용이성 확보 &amp;amp; 유지보수 향상 필요⇒ 다양한 PyTorch Template이 있으니 활용AutoGrad &amp;amp; Optimizertorch.nn.Module딥러닝을 구성하는 Layer의 base class Input, Ouput, Forward, Backward, 학습 대상이 되는 parameter(tensor) 정의nn.ParameterTensor 객체의 상속 객체 nn.Module 내에 attribute가 될 때는 ‘required_grad=True’로 지정되어 AutoGrad의 대상이 됨 우리는 이미 있는 레이어를 사용하는 경우가 많기 때문에 직접 선정할 일은 거의 X → 대부분의 layer에는 weights 값들이 지정되어 있음BackwardLayer에 있는 Parameter들의 미분 수행 Forward의 결과 값(모델의 예측값)과 실제 값간의 차이(loss)에 대해 미분 → 해당 값으로 Parameter 업데이트## 꼭 수행해야하는 단계 (in epochs)....optimizer.zero_grad() # optimizer 초기화outputs = model(inputs) # model에 inputs을 넣어 예측 값 생성loss = criterion(outputs, labels) # 예측값(outputs)과 실제값(labels)을 기준으로 loss 확인print(loss) loss.backward() # loss에 대해 미분optimizer.step() # 파라미터 업데이트 Dataset &amp;amp; DataloaderDataset 클래스하나의 데이터에 어떻게 가져올 것인가? 데이터 입력 형태를 정의하는 클래스 데이터 입력하는 방식의 표준화 Step __init__() : 초기 데이터 생성 방법 지정 __len__() : 데이터의 전체 길이 __getitem__() : index 값을 주면 반환되는 데이터의 형태 (X, y) 생성시 유의점 데이터 형태에 따라 각 함수를 다르게 정의 모든 것을 데이터 생성 시점에 처리할 필요는 없음 (image의 Tensor 변환은 학습에 필요한 시점에) Data set에 대한 표준화된 처리방법 제공 필요 DataLoader 클래스index를 가지고 여러개의 데이터를 한 번에 묶어서 모델에 던짐 Data의 Batch를 생성해주는 클래스 → Tensor 변환 + Batch 처리가 메인 업무 학습 직전 데이터의 변환을 책임#https://pytorch.org/docs/stable/data.htmlDataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, *, prefetch_factor=2, persistent_workers=False)# sampler : 데이터를 어떻게 뽑을지 index를 정하는 기법# collate_fn : Variable length의 가변인자들을 다룰 때 사용" }, { "title": "[부스트캠프 AI] Day6", "url": "/posts/Day6/", "categories": "Boostcamp AI, Python, PyTorch", "tags": "Boostcamp AI", "date": "2022-01-24 00:00:00 +0900", "snippet": "[PyTorch]Introduction to PyTorch딥러닝 프레임워크 리더는 2개 TensorFlow - Google PyTorch - FacebookKeras&amp;amp;TensorFlow와 PyTorch의 차이Computational Graph : 연산 과정을 그래프로 표현하는 것 Keras&amp;amp;TensorFlow - “Define and Run” → 실행시점에 데이터를 Feed PyTorch - “Define by Run” (Dynamic Computational Graph, DCG) → 실행을 하면서 그래프를 생성하는 방식PyTorch[장점] Define by Run, 즉시 확인이 가능함 → Pythonic code 가능 Numpy 구조를 가지는 Tensor 객체로 표현, 기초 문법이 Numpy와 비슷 AutoGrad(자동미분) : DL 연산을 지원, 딥러닝 프레임워크의 핵심 Function(다양한 딥러닝 함수) : 다양한 DL 함수, 모델 지원 → 복잡한 것들을 쉽게 할 수 있음PyTorch BasicsTensor 다차원 Arrays를 표현하는 PyTorch 클래스 Numpy의 ndarray와 동일, 함수도 거의 동일 기본적으로 tensor가 가질 수 있는 data type은 numpy와 동일 (GPU에 올려서 사용 가능하다는 차이) → data.device() torch.cuda.is_avilable()# data to tensordata = [[3, 5], [10, 5]]x_data = torch.tensor(data)# ndarray to tensornd_data = np.array(data)tensor_array = torch.from_numpy(nd_array_ex) torch.Tensor와 torch.tensor의 차이 torch.Tensor : Class. FloatTensor의 별칭 (int type을 넣으면 → Float로 변환) torch.tensor : Function. (int type을 넣으면 int 그대로 반환) Tensor handling torch.Tensor.view() : reshape과 동일, tensor의 shape 변환 (view 사용 권장) torch.Tensor.squeeze() : 차원의 개수가 1인 차원 삭제 (압축) torch.Tensor.unsqueeze() : 차원의 개수가 1인 차원 추가 → index 넣어주면 그 위치에 추가Tensor operations 기본적인 operations - numpy와 동일 (+) : torch.add(), (-) : torch.sub(), (*) : torch.mul(), (/) : torch.div() 행렬곱셈 연산은 함수 dot이 아닌 mm 사용 torch.mm : 행렬 차원이 맞아야 연산 (Broadcasting X) torch.matmul : 행렬 차원이 맞지 않아도 연산 가능 (Broadcasting O) nn.functional : 다양한 수식 변환 지원 (softmax, onehot~) torch.index_select(*input, dim, index*) : 인덱싱 torch.gather(*input, dim, index*) : 인덱싱 - 요소 가져오기 #https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 AutoGradPyTorch의 핵심. 자동 미분 .backward() 함수 사용nn.Module nn.Sequential() nn.ModuleDict() Parameter(torch.ones((out_, in_))) : 1로 초기화 Buffer" }, { "title": "[부스트캠프 AI] Week1 회고", "url": "/posts/Week1-%ED%9A%8C%EA%B3%A0/", "categories": "Boostcamp AI", "tags": "Boostcamp AI", "date": "2022-01-23 00:00:00 +0900", "snippet": "부스트캠프 1주차 회고💡 느낀 점우선, 생각했던 것보다 더 정신없고 힘든 첫 주였다. 강의량 자체도 많고, 밋업하는 시간도 상당해서 코어타임에만 학습하면 부족해서 식사 시간이나 코어타임 이후에도 강의를 계속 듣고, 기록했다. (깃허브 블로그에도 정리하고 싶었는데,, 주말에 한 번 더 정리해서 올리는 방향으로 해야겠다.)💡 좋았던 점피어세션을 통해서 함께 공부할 수 있다는 점이다. 물론 첫주라 체계적으로 시간을 활용하지는 못했다는 아쉬움이 있지만, 다들 열정적이고, 소통을 중요하게 생각하는 편이라 좋았고, 체계는 차차 갖춰나갈 것이라고 생각한다. 앞으로가 더 기대된다.💡 아쉬웠던 점개인적인 금주 학습에서 아쉬웠던 점은 강의 듣는 것과 학습 기록이라는 행동 자체에 목표가 더 있었다는 점이다. 완벽이란 없지만 제대로 이해했나?라는 질문을 던졌을 때 확실한 대답을 하지는 못할 것 같다. 남은 오늘과 주말을 이용해서 기존에 가지고 있는 ML/DL 지식들과 연결해보고, 나의 언어로 다시 정리해보자. 💡 정리기존에 하고 있던 취업 스터디 또한 시간이 많이 소요되어 더 힘들었다고 생각하는데, 차주부터는 취업 스터디 관련된 시간을 줄이기도 했고, 이번주의 하루씩 지날때마다 몸도 적응하고 있다. 다음주는 조금 더 시간베이스로 목표를 잘 설정하고 여유를 가질 수 있을 것이라 기대한다." }, { "title": "[부스트캠프 AI] Day5", "url": "/posts/Day5/", "categories": "Boostcamp AI, Python", "tags": "Boostcamp AI", "date": "2022-01-21 00:00:00 +0900", "snippet": "[Python]5-1. File / Exception / Log HandlingException 분류 예상 가능한 예외 : 개발자가 명시적으로 정의 (ex. 사용자의 잘못된 입력, 파일 없음) 예상 불가능한 예외 : 인터프리터 과정에서 발생하는 예외 → 발생시 인터프리터가 자동 호출 (ex. 개발자 실수) 예외처리 [try~except] *학습자료와 다른 예시try: a = [1,2] print(a[3]) 4/0except ZeroDivisionError: print(&quot;0으로 나눌 수 없습니다.&quot;)except IndexError: print(&quot;인덱싱 할 수 없습니다.&quot;)# else: -&amp;gt; 예외 발생X 시 동작하는 코드# finally: -&amp;gt; 예외 발생 여부와 관련X 실행 종류 - IndexError, NameError, ZeroDivisionError, ValueError, FileNotFoundError 구문 try ~ except / try ~ except ~ else / try ~ except ~ finally raise exception_info → 강제로 예외 발생 assert 예외조건 → 조건에 만족X 경우 예외 발생 File 파일의 기본 체계 - 파일(정보 저장하는 논리적 단위, 확장자로 식별) / 디렉토리(폴더) 종류 Binary 파일 : 컴퓨터만 이해 가능한 이진법 형식 (ex. 엑셀, 워드 파일 등) Text 파일 : 사람도 이해 가능한 문자열 형식 (ex. HTML, Python 코드 파일 등) I/O - ‘open’ 사용 [mode : r(읽기), w(쓰기), a(추가)] read() : 같은 폴더에 있을 경우, 파일 내용을 문자열로 반환 / readlines() : 한줄씩 읽어 list type으로 반환 Log 파일 생성 - 디렉토리가 있는지, 파일 있는지 확인 후 Pickle - 파이썬 객체를 영속화(persistence) 하는 bulit-in 객체, 활용 다양함 → 데이터 크기 클 때 pickle로 저장하면 크기가 작아져서 많이 사용했음Logging프로그램 실행동안 일어나는 정보 기록 (유저 접근, Exception, 특정 함수 사용 등) Colsole 창에만 남는 기록은 분석시 사용 불가 레벨별(개발, 운영)으로 기록 / 모듈별 별도 logging 모듈 사용 → level : debug &amp;gt; info &amp;gt; warning &amp;gt; error &amp;gt; critical프로그램 실행 설정 configparser - file에 실행 설정 저장 (Section, Key, Value 값 형태 설정파일, Dict Type으로 호출 후 사용) argparser - 프로그램 실행시 Setting 정보 저장 (Command-Line Option)5-2. data handlingCSV csv 객체 활용 [delimiter(글자나눔), lineterminator(줄바꿈), quotechar(문자열 둘러싸는 신호 문자), quoting(데이터 나누는 기준이 quotechar에 의해 둘러싸인 레벨)]Web : 요청 → 처리 → 응답 → 렌더링 HTML(Hyper Text Markup Language) 정규식(regular expression) : 복잡한 문자열 패턴을 정의하는 문자 표현 공식 → 정규식 연습장 “http://www.regexr.com/” XML : 데이터의 구조와 의미를 설명하는 TAG를 사용하여 표시하는 언어 TAG와 TAG 사이에 값이 표시, 구조적인 정보 표현 가능 스키마, DTD 등으로 메타정보 표현 컴퓨터간에 정보를 주고받기 매우 유용한 저장방식으로 쓰임 parsing : beautifulsoup으로 파싱 JSON : Java Script의 데이터 객체 표현 방식 간결성(이해 편함), 데이터 용량 적고 Code로의 전환 쉬움 key:value 쌍으로 데이터 표시 각 사이트마다 Developer API 활용법 찾아 사용 6. numpynumpy일반 List에 비해 빠르고, 메모리 효율적, 반복문X 데이터 배열에 대한 처리 지원. Matrix, Vector와 같은 Array 연산의 사실상의 표준 ndarray : np.array 함수 활용, 배열 생성 (list와 차이 : dynamic typing not supported - 다양한 타입 넣을 수 X) array creation : .shape (dimension 구성) / .dtype (데이터 타입) array shape0(Rank) : scalar → 1 : vector → 2 : matrix → 3 : 3-tensor → n : n-tensor reshape() : element의 개수는 동일, shape 크기 변경 *reshape(-1, n) : 전체 element size를 기반으로 개수 선정 flatten() : 다차원 array → 1차원indexing &amp;amp; slicing 2차원 배열에서 [0(row), 0(col)] 지원 *list는 [0][0]만 가능 행&amp;amp;열 부분을 나눠서 slicing 가능 *list는 불가능creation function np.arange() : array의 범위를 지정, 지정한 값의 배열 추출 np.zeros(shape, dtype, order) : 0으로 가득찬 ndarray 생성 np.ones(shape, dtype, order) : 1로 가득찬 ndarray 생성 np.empty() : shape만 주어지고 비어있는 ndarray(메모리 초기화X) np.identitly(n, dtype) : 단위행렬 생성 np.eye() : 대각선이 1인 행렬, 시작 index 변경 가능 np.diag() : 대각 행렬의 값을 추출 np.random.분포종류() : 데이터 분포에 따른 sampling으로 array 생성operation functions sum/ axis / mean / std … vstack / hstack : numpy array를 합치는 함수 (가로, 세로) np.concatenate() : numpy array를 합치는 함수 (가로, 세로)array operations +, -, *(array간 shape이 같을 때) .dot() : Matrix의 기본연산 (행렬곱) .transpose() : 전치행렬 np.argmax() , np.argmin() : array내 최대값 또는 최소값의 index 반환numpy data i/o np.loadtxt() / np.savetxt()7-1. Pandas_1 Series / DataFrame - loc(index 이름), iloc(index number)→ series - index 기준으로 연산 수행(없으면 NaN) / DataFrame - column, index 모두 고려 lambda, map, apply series type에도 map 사용 가능, function 대신 dict, sequence형 등으로 자료형 대체 가능 bulit-in : describe, unique, sum, isnull, sort_values, corr, cov 7-2. Pandas_2Groupbysplit → apply → combine 과정을 거쳐 연산 Hierarchical index 두 개의 column으로 groupby 할 경우 index가 두 개 생성 unstack() : group으로 묶인 데이터 → matrix 형태 swaplevel() : index level 변경 인덱스 기준으로 기본연산 수행 가능 grouped Groupby에 의해 Split된 상태 추출 가능 .get_group() : 특정 key 값을 가진 그룹의 정보만 추출 가능 aggregation() or agg() / transformation() - 변환시 / filtration() - 조건 검색 Pivot table &amp;amp; Crosstab Pivot table : column에 추가로 labeling 값 추가, value에 numeric type 값을 aggregation하는 형태 Crosstab : 특히 두 칼럼의 교차 빈도, 비율, 덧셈 등을 구할 때 사용, Pivot table의 특수한 형태 merge &amp;amp; concat join method : INNER JOIN(교집합), FULLJOIN(합집합), LEFT &amp;amp; RIGHT JOIN persistence DB connection : Data loading 시 db connection 기능 제공 import sqlite3 XLS persistence df의 엑셀 추출 코드. xls 엔진으로 openpyxls 또는 xlsxwrite 사용 wt = pd.ExcelWriter() → df.to_excel(wf) Pickle persistence .to_pickle(), .read_pickle() [AI Math]10. RNN 첫걸음시퀀스 데이터 이해하기시퀀스(sequence) 데이터 : 소리, 문자열, 주가 등의 데이터 독립동등분포(i.i.d) 가정 잘 위배 → 순서 바꾸거나 과거 정보에 손실 발생하는 경우 확률분포도 바뀜시퀀스 데이터 다루기 조건부확률(베이즈법칙) 이용 → 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포 다루기 위함 시퀀스 데이터 분석시 모든 과거 정보들이 필요한 것은 X 길이가 가변적인 데이터를 다룰 수 있는 모델→ 조건부에 들어가는 데이터 가변적 AR($\\tau$) 자귀회귀모델(Autoregressive Model) : 고정된 길이 $\\tau$만큼의 시퀀스만 사용하는 경우 *$\\tau$ : 하이퍼 파라미터 - 사전 지식 필요, 문제에 따라 $\\tau$ 바뀔 수 있음 잠재 AR 모델 : 이전 정보를 제외한 나머지 정보를 $H_t$라는 잠재변수로 인코딩해서 활용 장점 과거 모든 데이터 활용 가변적 데이터 길이 → $\\tau$, $H_t$라는 고정된 데이터 길이로 바꿀 수 있음 문제점 과거의 데이터를 어떻게 잠재변수로 인코딩? ⇒ 해결 위해 RNN 등장 RNN(Recurrent Neural Network) 이해하기  가장 기본적인 RNN 모형 → MLP와 유사한 모양[ ▲ MLP와 유사한 모양의 모델 ] ⇒ 기본적인 MLP 모델로는 과거의 정보를 다룰 수 없음 → 입력행렬이 t번째만 있기 때문 [ ▲ 잠재변수 인코딩하는 모델 ] 모델의 특징 잠재변수인 $H_t$를 복제 → 다음 순서의 잠재변수 인코딩에 활용 RNN 모델링 : 이전 순서의 잠재변수 + 현재의 입력 RNN의 역전파 : 잠재변수의 연결그래프에 따라 순차적으로 계산 BPTT(Backpropagation Through Time) 가중치행렬의 미분을 계산 → 미분의 곱으로 이루어진 항 계산됨 시퀀스 길이가 길어지는 경우 BPTT를 통한 역전파 알고리즘의 계산이 불안정해짐 → 기울기 소실문제 기울기 소실의 해결책  truncated BPTT : 시퀀스 길이가 길어지는 경우 BPTT는 기울기 소실의 문제가 생김 → 길이를 끊는 것 필요 기울기 소실의 문제 해결을 위해 등장한 네트워크 ⇒ LSTM, GRU" }, { "title": "[부스트캠프 AI] Day4", "url": "/posts/Day4/", "categories": "Boostcamp AI, Python", "tags": "Boostcamp AI", "date": "2022-01-20 00:00:00 +0900", "snippet": "[AI Math]6. 확률론 맛보기딥러닝에 확률론이 필요한 이유딥러닝 : 확률론 기반의 기계학습 이론에 바탕 손실함수(loss function)들의 작동원리 → 데이터 공간을 통계적으로 해석해서 유도 분산 및 불확실성을 최소화하기 위해서는 측정하는 방법을 알아야함 ⇒ 측정 방법을 통계학에서 제공 💡 기계학습의 기본 원리 : 예측이 틀릴 위험을 최소화하도록 데이터를 학습하는 원리 1) 회귀분석의 손실함수 L2-노름   → 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도 2) 분류 문제의 교차엔트로피(cross-entropy)  → 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도확률분포는 데이터의 초상화데이터 공간을 $x$ x $y$라 표기하고 $D$는 데이터 공간에서 데이터를 추출하는 분포 💡 실제 데이터만을 가지고 $D$를 아는 것은 불가능 → 기계학습 모형을 가지고 $D$를 추론 $(x, y) \\in x$ x $y$ 는 데이터 공간 상의 관측 가능한 데이터에 해당 데이터는 확률변수로 $(x, y) \\sim D$라 표기확률변수확률분포 $D$의 종류에 따라 이산형(discrete), 연속형(continuous)확률변수로 구분 이산확률변수 확률 변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링 $\\mathbb{P} (X\\in A) = \\sum_{x\\in A} P(X = \\mathbf x)$ 연속형확률변수 데이터 공간에 정의된 확률변수의 밀도(density)위에서의 적분을 통해 모델링 $\\mathbb{P} (X\\in A) = \\int_{A} P(X)\\mathbf {dx}$ *밀도는 누적확률분포의 변화율을 모델링하며, 확률로 해석하면 안됨 결합분포(joint destribution) 원래 확률 분포에 상관없이 결합분포 사용 가능 (모델링 방법에 따라 달라짐) 주변확률분포 $P(\\mathbf x)$ → 결합분포 $P(\\mathbf x, y)$에서 유도 가능 *y에 대한 정보를 주지는 않음 조건부확률입력변수 $\\mathbf x$에 대해 정답이 $y$일 확률 (연속확률분포의 경우 확률이 아닌 밀도로 해석) 선형모델과 소프트맥스 함수의 결합 → 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용 💡 분류문제에서 $softmax (W\\phi+\\mathbf b)$   : 데이터 $\\mathbf x$로부터 추출된 특징패턴 $\\phi (\\mathbf x)$과 가중치행렬 $W$을 통해 조건부확률 $P (y|\\mathbf x)$ 계산 회귀문제   : 조건부 기대값 $\\mathbb E[y|\\mathbf x]$ 추정    - 조건부 기대값은 $\\mathbb E || y-f(x)||_2$를 최소화하는 함수 $f(x)$와 일치기대값데이터를 대표하는 통계량 확률분포를 통해 다른 통계적 범함수 계산에 사용 확률 분포가 주어지면 데이터를 분석하는데 사용 가능한 여러 통계적 범함수(statistical functional)를 계산 가능 분산, 첨도, 공분산 등 계산 가능몬테카를로 샘플링 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로 샘플링 방법 사용 독립추출만 보장된다면 대수의 법칙(law of large number)에 의해 수렴성 보장 방법 1) 타겟 $f(x)$에서 $x$자리에 샘플링한 데이터 대입 2) 데이터들에 따라서 $f(x)$의 산술평균 계산 (원하는 기대값에 근사) ❗️ 독립추출 - 샘플링하는 분포에서 독립적으로 샘플링 해주어야함기대값, 분산의 계산 이산균등분포의 기대값 $\\mathbb E_{x\\sim P(X)}[f(\\mathbf x)] = \\sum _{\\mathbf x \\in X} f(\\mathbf x)P(\\mathbf x)$ 💡 $X \\in {1, 2, 3, 4}$ 인 경우, $X$의 기대값?   ⇒ $1 * (1/4) + 2 * (1/4) + 3 * (1/4) + 4 * (1/4)$ = 2.5  이산균등분포의 분산 $V(\\mathbf x) = \\mathbb E [X^2] - \\mathbb E [X]^2$ 💡 $X \\in {1, 2, 3, 4}$ 인 경우, $X$의 분산? ⇒ $1^2 * (1/4) + 2^2 * (1/4) + 3^2 * (1/4) + 4^2 * (1/4)$ - $(2.5)^2$ = 1.257. 통계학 맛보기모수(parameter)정규분포로 확률분포를 모델링할 경우 → 모수 : 평균, 분산 💡 통계적 모델링의 목표 적절한 가정 위에서 확률분포를 추정(inference)하는 것 기계학습, 통계학이 공통적으로 추구하는 목표 예측 모형의 목적 데이터와 추정 방법의 불확실성을 고려, 위험을 최소화 유한한 개수의 데이터 관찰로 모집단의 분포를 정확하게 알기는 불가능 ⇒ 근사적으로 확률분포 추정 모수적(parametric)방법론 데이터가 특정 확률분포를 따른다고 가정한 후 그 분포를 결정하는 모수를 추정하는 방법 비모수(nonparametric)방법론 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌는 방법 *모수가 없다X, 쓰지 않는다X 확률분포 가정하기  히스토그램을 통해 모양을 관찰 베르누이 분포 : 데이터가 2개의 값(0 또는1)만 가지는 경우 카테고리 분포 : 데이터가 n개의 이산적인 값을 가지는 경우 베타분포 : 데이터가 [0, 1] 사이에서 값을 가지는 경우 감마분포, 로그분포 등 : 데이터가 0이상의 값을 가지는 경우 정규분포, 라플라스분포 등 : 데이터가 $\\mathbb R$ 전체에서 값을 가지는 경우    ❗️기계적으로 가정X, 데이터를 생성하는 원리는 먼저 고려    ❗️모수를 추정한 후에 반드시 검정데이터로 모수 추정하기데이터의 확률분포를 가정했다면 모수 추정 가능 정규분포의 모수 : 평균 $\\mu$과 분산 $\\sigma^2$ 💡 표본분산을 구할 때 N이 아니라 N-1로 나누는 이유 불편(unbiased) 추정량을 구하기 위해 💡 중심극한정리(Central Limit Theorem) 표본평균의 표집분포는 N이 커질수록 정규분포 $N(\\mu, \\sigma^2 / N)$를 따름 *표집분포(표본평균의 확률분포) ≠ 표본분포 최대가능도 추정법(maximum likelihood estimation, MLE)이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나 데이터 집합 $\\mathbf X$가 독립적으로 추출되었을 경우 로그가능도를 최적화 💡 Why 로그 가능도 사용? 데이터가 많아진다면 컴퓨터의 정확도로 가능도를 계산하는 것이 불가능(오차 때문) 로그함수의 성질 : 곱을 덧셈으로 바꾸어줌 ⇒ 컴퓨터로 연산 가능 연산량 $O(n^2)$ → $O (n)$ 대게의 손실함수의 경우 경사하강법 사용 ⇒ 음의 로그가능도 최적화 딥러닝에서 최대가능도 추정법최대가능도 추정법을 이용하여 기계학습 모델 학습 가능 딥러닝 모델의 가중치를 $\\theta = (\\mathbf W^{(1)} , …, \\mathbf W^{(L)})$라 표기했을 때, 분류문제에서 소프트맥스 벡터는 카테고리분포의 모수 $(p_1, …, p_k)$를 모델링 One-hot 벡터로 표현한 정답레이블을 관찰데이터로 이용 → 확률분포인 소프트맥스 벡터의 로그 가능도 최적화 가능확률분포의 거리  기계학습에서 손실함수 → 모델이 학습하는 확률분포 - 데이터에서 관찰되는 확률분포의 거리를 통해 유도 두 확률분포 사이의 거리(distance) 계산 총 변동 거리(Total Variation Distance, TV) 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL) 분류문제에서 정답레이블을 $P$, 모델 예측을 $Q$라 두는 경우 최대 가능도 추정법 = 쿨백-라이블러 발산 최소화 바슈타인 거리(Wasserstein Distance) 8. 베이즈 통계학 맛보기베이즈 정리 : 데이터가 새로 추가되었을 때 정보를 업데이트하는 방식에 대한 기반이 됨조건부 확률조건부확률 $P(A|B)$ : 사건 $B$가 일어난 상황에서 사건 $A$가 발생할 확률$P(A \\cap B) = P(B)P(A|B)$ 💡 A라는 새로운 정보가 주어졌을 때 $P(B)$로 부터 $P(B|A)$를 계산하는 방법 $P(B\\|A) =$ $P(A\\cap B)\\over P(A)$ $= P(B)$ ${P(A\\|B)}\\over P(A)$베이즈 정리 조건부 확률과 인과관계조건부 확률을 인과관계(causality)를 추론할 때 함부로 사용X 인과관계 : 데이터 분포의 변화에 강건한 예측 모형을 만들 때 필요 중첩요인(confounding factor)의 효과를 제거, 원인에 해당하는 변수만의 인과관계 계산 필요 *효과 제거하지 않으면 가짜 연관성(spurious correlation)이 나옴 9. CNN 첫걸음Convolution 연산 이해하기MLP → 각 뉴런들이 선형모델과 활성함수로 모두 연결된(fully connected) 구조 커널(kernel)을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조 Convolution 연산의 수학적인 의미   : 신호(signal)를 커널을 이용해 국소적으로 증폭 또는 감소시켜 정보를 추출 또는 필터링하는 것 커널은 정의역 내에서 움직여도 변하지X, 주어진 신호에 국소적(local)으로 적용 Convolution 연산은 1차원뿐만 아니라 다양한 차원에서 가능  💡 $i, j, k$가 바뀌어도 커널 $f$의 값은 바뀌지 X2차원 Convolution 연산  2D-Conv 연산은 커널(kernel)을 입력벡터 상에서 움직여가며 선형모델과 합성 함수가 적용되는 구조 💡 출력 크기의 계산   입력 크기 $(H, W)$, 커널 크기 $(K_H, K_W)$, 출력 크기 $(O_H, O_W)$ 일 때,   👉 $O_H = H - K_H + 1$ , $O_W = W - K_W +1$ 채널이 여러개인 경우 커널의 채널 수와 입력의 채널 수가 같아야함 3차원부터는 행렬이 아니라 ‘텐서’라고 부름 Convolution 연산의 역전파 Convolution 연산은 역전파 계산시에도 Convolution 연산이 나옴 → 커널이 모든 입력데이터에 공통으로 적용되기 때문 역전파 단계에서 다시 커널을 통해 gradient가 전달 → $\\delta_1w_3 + \\delta_2w_2 + \\delta_3W_1$ 커널에는 $\\delta$(미분값)에 입력값 $x_3$을 곱해서 전달 각 커널에 들어오는 모든 gradient를 더하면 결국 convolution 연산과 같음" }, { "title": "[부스트캠프 AI] Day3", "url": "/posts/Day3/", "categories": "Boostcamp AI, Python", "tags": "Boostcamp AI", "date": "2022-01-19 00:00:00 +0900", "snippet": "[Python]3-1. Python Data Structure Stack(LIFO), Queue(FIFO) tuple : 값 변경이 불가능한 리스트, 데이터 변경X 경우 값이 하나인 tuple은 ‘,’ 붙여야 튜플로 인식 set : 중복이 불가능한 자료, 순서X dictionary(Hash Table) - key:value Collection Module [collections] deque : Stack, Queue 지원하는 모듈 list에 비해 효율적인 저장방식 지원 Counter : data element들의 갯수를 dict 형태로 반환 OrderedDict : 3.6부터 기본 지원 defaultdict : Dict의 값에 기본 값을 지정, 신규값 생성시 사용하는 방법 👉defaultdict 이용 예시 namedtuple : tuple 형태로 Data 구조체를 저장 (data의 variable을 사전에 지정해서 저장) 3-2. Pythonic code split &amp;amp; join list comprehension* : 일반적으로 for + append보다 속도 빠름 enumerate &amp;amp; zip lambda &amp;amp; map &amp;amp; reduce generator asterisk4-1. Python Object Oriented Programming 💡 OOP : 객체 개념을 프로그램으로 표현 - 속성은 변수(variable), 행동은 함수(method)로 표현 - Class : 설계도, instance : 실제 구현체 Python naming rule : snake_case, CamelCase Attribute 추가 : __ init__(객체 초기화 함수), self(추가해야만 class 함수로 인정됨) Notebook : 사용자는 Note에 뭔가를 적을 수 있음. Content가 있고, 내용 제거 가능 Inheritance(상속) : 부모클래스로부터 속성과 Method를 물려받은 자식 클래스를 생성하는 것 Polymorphism(다형성) : 같은 이름 메소드의 내부 로직을 다르게 작성 Visibility(가시성) : 객체 정보를 볼 수 있는 레벨 조절 → 누구나 객체 안에 모든 변수를 볼 필요가 없음 💡 Encapsulation(캡슐화, 정보은닉) - Class 설계할 때, 클래스 간 간섭/정보공유의 최소화 decorate First-class objects(일등함수) : 변수나 데이터 구조에 할당이 가능한 객체, 파라미터로 전달 가능 + 리턴 값으로 사용 Inner function : 함수 내에 또 다른 함수가 존재 closures : inner function을 return 값으로 반환 4-2. Module and ProjectModule프로그램을 모듈화 시키면 다른 프로그램이 사용하기 쉬움 같은 폴더에 Module에 해당하는 .py 파일 &amp;amp; 사용하는 .py 저장 → import로 호출 namespace : 필요한 내용만 호출 가능 Alias 설정 import numpy as np 모듈에서 특정 함수 또는 클래스만 호출 from itertools import Combinations 모듈에서 모든 함수 또는 클래스 호출 from sklearn.metrics import * Built-in Model : random, time, urllib.request Package하나의 대형 프로젝트를 만드는 코드의 묶음, 다양한 모듈들의 합(폴더로 연결) Package 만들기 1) 기능을 세부적으로 나눠 폴더 만듦 2) 각 폴더별로 필요한 모듈 구현 3) 1차 Test (python shell에서) 4) 폴더별로 __ init__.py 구성하기 5) __ main__.py 파일 만들기 * __ init__.py : 현재 폴더가 패키지임을 알리는 초기화 스크립트 하위 폴더와 .py파일(모듈)을 모두 포함Virtual Environment프로젝트 진행 시 필요한 패키지만 설치하는 환경 다양한 패키지 관리 도구를 사용 virtualenv (+pip): 가장 대표적인 가상환경 관리 도구 (레퍼런스+패키지 개수) conda : 상용 가상환경 도구 → windows에서 설치 장점 Windows : conda / linux, mac : conda or pip [AI Math]5. 딥러닝 학습방법 이해하기신경망을 수식으로 분해해보기  ⇒ 비선형모델인 신경망(neural network) 행벡터 $O_i$는 데이터 $X_i$와 가중치 행렬 $W$사이의 행렬곱과 절편 $b$벡터의 합으로 표현 $X_i$ : 데이터 , $W$: 가중치 행렬(다른 데이터 공간으로 보내줌), $b$ : y절편(각 행이 같음) 출력 차원 : n x p ((n x d) x (d x p)) softmax 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산 출력 벡터 o에 softmax 함수를 합성하면 확률벡터가 되므로 ‘특정 클래스 k에 속할 확률’로 해석 가능 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측 $softmax(o) = softmax(W_x+b)$ 신경망 : 선형모델과 활성함수(activation fuction)를 합성한 함수활성함수(activation function) 활성함수를 쓰지 않으면 딥러닝은 선형모델과 차이가 없음 sigmoid, tanh, ReLU (나중에 자료 추가)다층 퍼셉트론(MLP)신경망이 여러층 합성된 함수 MLP의 파라미터는 L개의 가중치 행렬 $W^1$~$W^n$과 $b^1$~$b^n$으로 이루어져 있음 $l = 1, …, L$까지 순차적인 신경망 계산 ⇒ 순전파 (학습X 주어진 데이터 → 출력으로 내뱉는 것) 층을 여러개 쌓는 이유? 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 빠르게 줄어들어 효율적인 학습 가능 층이 얇으면 필요한 뉴런의 숫자가 기하급수적으로 늘어나서 넓은 신경망이 되어야함 딥러닝 학습 원리 : 역전파(backpropagation) 딥러닝은 역전파 알고리즘을 이용하여 각 층에 사용된 파라미터 ${W^{(l)}, b^{(l)}}^L_{l=1}$를 학습 손실함수를 $l$라 했을 때 $dl/dw^{(l)}$ 정보 계산에 사용 $l = L, …, 1$ 순서로 연쇄법칙을 통해 그래디언트 벡터를 전달 미분 한번에 계산X, 순차적 계산 (윗층부터 역순으로)추가 정리 자료 tanh 미분import numpy as npdef tanh(x, diff=False): if diff: return (1+tanh(x))*(1-tanh(x)) else: return np.tanh(x)" }, { "title": "[부스트캠프 AI] Day2", "url": "/posts/Day2/", "categories": "Boostcamp AI, Python", "tags": "Boostcamp AI", "date": "2022-01-18 00:00:00 +0900", "snippet": "[Python]2-2. Function and Console I/O GUI, CLI console in/out : input(), print() formating : %, str, padding, naming, f-string2-3. Conditionals and Loops반복문 변수명 : i, j, k (x, y, z와 같은 관례) 0부터 시작 (2진수가 0부터 시작) 무한 loop : CPU, 메모리 등 컴퓨터 리소스 과다하게 점유하니 무한 loop에 빠지지 않도록 주의2-4. String and adcanced function concept 함수 호출 방식 : Call by Value(값만 넘김), Call by Reference(메모리 주소 넘김) 변수의 범위 : 지역변수(local variable) - 함수내 / 전역변수(Global variable) - 프로그램 전체 docstring : 함수에 대한 상세스펙을 사전에 작성 → 사용자 이행도 UP ⇒ VS code ‘docstring generator’ 함수 개발 가이드라인 공통적으로 사용되는 코드, 복잡한 수식, 복잡한 조건 ⇒ 함수화 다른 사람이 쉽게 사용할 수 있도록 작성 규칙 : 코딩 컨벤션(구글 파이썬 컨벤션 찾아보기) 체크 : ‘flacke8’모듈 - 확인 가능 / ‘black’ 모듈 - 자동 변경 가능 [AI Math]3. 경사하강법 - 순한맛미분(differentiation) sympy.diff변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구 최적화에서 가장 많이 사용 변화율(기울기)의 극한(limit)으로 정리미분의 이해미분은 함수 f의 주어진 점 $(x, f(x))$에서의 ‘접선의 기울기’ 경사상승법(gradient ascent) : 미분값 더하기. 함수 극대값의 위치 구함(목적함수 최대화) 경사하강법(gradient descent) : 미분값 빼기. 함수 극소값의 위치 구함(목적함수 최소화) 극값에 도달 시 멈춤 ⇒ 극값에서는 미분 값 0경사하강법(gradient descent) Input : gradient(미분 계산 함수), init(시작점), lr(학습률), eps(알고리즘 종료조건) Output : var 벡터가 입력인 다변수 함수 ⇒ 편미분(partial differentiation) 사용 각 변수별 편미분 계산한 gradient vector 이용하여 경사하강/상승법에 사용 가능 알고리즘은 그대로 적용, 절대값 대신 노름(norm)계산해서 종료조건 설정 4. 경사하강법 - 매운맛선형회귀 선형회귀의 목적식($||y-X\\beta||_2$)을 최소화하는 $\\beta$ 찾는 것이 목적 $\\beta$ 최소화 : 목적식을 $\\beta$로 미분 → 주어진 $\\beta$에서 미분 값 빼줌 ⇒ 경사하강법 L2-노름의 제곱 사용 : L2-노름을 최소화 하는 $\\beta$ 찾기, L2-노름의 제곱 최소화하는 $\\beta$ ⇒ 같은 결과 값경사하강법 기반 선형 회귀 알고리즘 Input : X, y, lr(학습률), T(학습횟수) *중요한 hyperparameter Output : beta $\\bigtriangledown_\\beta||y-X\\beta||^2_2$ 항 계산하여 $\\beta$ 업데이트 ⇒ 무어펜로즈 역행렬 이용X 계수 구할 수 있음 lr 큰 경우 : 불안정 / 작은경우 : 정답 찾지 못하고 끝경사하강법, SGD 경사하강법 : 미분 가능, 볼록한 함수 → 적절한 학습률, 학습횟수 선택시 수렴 보장 비선형회귀의 경우 목적식이 대부분 볼록함수가 아니어서 항상 보장 되지 않음 ⇒ 변형된 경사하강법 사용 확률적경사하강법(stochastic gradient descent, SGD) : 모든 데이터X, 한개 또는 일부(mini-batch) 활용하여 업데이트 볼록이 아닌 목적식 → SGD를 통해 회적화 확률적으로 선택하므로 목적식 모양이 바뀌게 됨. 값은 다르겠지만 방향은 유사할 것으로 기대 일부만 사용하므로 알고리즘의 효율성, 하드웨어적으로 머신러닝 학습에 더 효율적 " }, { "title": "[부스트캠프 AI] Day1", "url": "/posts/Day1/", "categories": "Boostcamp AI, Python", "tags": "Boostcamp AI", "date": "2022-01-17 00:00:00 +0900", "snippet": "[Python]1-1. Basic computer class for newbies1-2. Python Overview1-3. Python Coding Environment [AI Math]1. Vector벡터(Vector)숫자를 원소로 가지는 리스트(list) 또는 배열(array) 공간에서 ‘한 점’을 나타냄 원점으로부터 상대적 ‘위치’ 표현벡터의 연산 스칼라곱 : 길이만 변함 덧셈, 뺄셈, 성분 곱 : 같은 모양을 가질 경우 가능벡터의 노름 (종류에 따라 기하학적 성질 달라짐) L1-노름 : 각 성분의 ‘변화량의 절대값’ L2-노름 : 피타고라스 정리를 이용해 ‘유클리드 거리’ 계산벡터 사이의 거리벡터의 뺄셈 이용, L1-노름&amp;amp;L2-노름 이용하여 계산 가능벡터 사이의 각도 계산제 2코사인 법칙 이용, L2-노름만 가능 내적 이용하여 분자 쉽게 계산 가능 np.inner 내적(inner production) : 정사영의 길이를 ‘벡터 y의 길이 ||y||만큼 조정’한 값 2. Matrix행렬(matrix)벡터를 원소로 가지는 2차원 배열 행(row), 열(column) 인덱스 가짐 전치행렬(transpose matrix) : 행과 열의 인덱스가 바뀐 행렬 ( nxm 행렬 → mxn 행렬) numpy에서는 행(row)이 기본 단위행렬의 연산 덧셈, 뺄셈 : 같은 모양을 가진 경우 가능 성분곱, 스칼라곱 : 벡터와 같음 행렬곱(matrix multiplication) : ‘i번째 행벡터와 j번째 열벡터 사이의 내적’을 성분으로 가지는 행렬 계산 numpy →@ 사용 X의 열의 개수와 Y의 행의 개수가 같아야함 행렬곱을 통해 ‘패턴 추출’, ‘데이터 압축’ 가능 행렬의 내적 np.inner(X, Y)X,Y의 행의 길이가 같아야 가능역행렬(inverse matrix) np.linalg.inv(X)어떤 행렬 A의 연산을 거꾸로 되돌리는 행렬, $A^{-1}$로 표시 n=m일때만 가능, 행렬 A의 행렬식이 0이 되면 안됨 무어-펜로즈(Moore-Penros) 역행렬 np.linalg.pinv(Y)   : 유사역행렬, $A^{+}$이용선형회귀분석np.linalg.pinv 이용, 선형회귀식 찾을 수 있음 *(n≥m)인 경우 : 데이터가 변수 개수보다 많거나 같아야함" } ]
