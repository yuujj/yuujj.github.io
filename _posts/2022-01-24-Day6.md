---
layout : post
title:  "[부스트캠프 AI] Day6"
excerpt: "2주차 학습기록"

categories: ["Boostcamp AI", "Python", "PyTorch"]
tags: ["Boostcamp AI"]

published: true
toc: true
toc_sticky: true
 
date: 2022-01-24
last_modified_at: 2022-01-30
use_math: true
---

# <b>[PyTorch]</b>
---

## **Introduction to PyTorch**

딥러닝 프레임워크 리더는 2개 

- TensorFlow - Google
- PyTorch - Facebook

<br>

### **Keras&TensorFlow와 PyTorch의 차이**
Computational Graph : 연산 과정을 그래프로 표현하는 것   
- Keras&TensorFlow - “Define and Run”     
    → 실행시점에 데이터를 Feed   
- PyTorch - “Define by Run” (Dynamic Computational Graph, DCG)   
    →  실행을 하면서 그래프를 생성하는 방식   
        
<br>

### **PyTorch**

**[장점]**
- Define by Run, 즉시 확인이 가능함 → Pythonic code 가능   
- Numpy 구조를 가지는 Tensor 객체로 표현, 기초 문법이 Numpy와 비슷   
- AutoGrad(자동미분) : DL 연산을 지원, 딥러닝 프레임워크의 핵심   
- Function(다양한 딥러닝 함수) : 다양한 DL 함수, 모델 지원 → 복잡한 것들을 쉽게 할 수 있음   

<br>

---
## **PyTorch Basics**

<br>

### **Tensor**
- 다차원 Arrays를 표현하는 PyTorch 클래스   
- Numpy의 ndarray와 동일, 함수도 거의 동일   
- 기본적으로 tensor가 가질 수 있는 data type은 numpy와 동일 (GPU에 올려서 사용 가능하다는 차이)   
    → `data.device()`  `torch.cuda.is_avilable()`   

```python
# data to tensor
data = [[3, 5], [10, 5]]
x_data = torch.tensor(data)

# ndarray to tensor
nd_data = np.array(data)
tensor_array = torch.from_numpy(nd_array_ex)
```
- torch.Tensor와 torch.tensor의 차이
    - torch.Tensor : Class. FloatTensor의 별칭 (int type을 넣으면 → Float로 변환)
    - torch.tensor : Function. (int type을 넣으면 int 그대로 반환)

<br>

### **Tensor handling**
- `torch.Tensor.view()` : reshape과 동일, tensor의 shape 변환 (view 사용 권장)   
- `torch.Tensor.squeeze()` : 차원의 개수가 1인 차원 삭제 (압축)   
- `torch.Tensor.unsqueeze()` : 차원의 개수가 1인 차원 추가 → index 넣어주면 그 위치에 추가   

<br>

### **Tensor operations**
- 기본적인 operations - numpy와 동일
- (+) : `torch.add()`, (-) : `torch.sub()`, (*) : `torch.mul()`, (/) : `torch.div()`
- 행렬곱셈 연산은 함수 dot이 아닌 mm 사용
    - `torch.mm` : 행렬 차원이 맞아야 연산 (Broadcasting X)
    - `torch.matmul` : 행렬 차원이 맞지 않아도 연산 가능 (Broadcasting O)
- nn.functional : 다양한 수식 변환 지원 (softmax, onehot~)
- `torch.index_select(*input, dim, index*)` : 인덱싱
- `torch.gather(*input, dim, index*)` : 인덱싱 - 요소 가져오기  
    ```python
    #https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather
    out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0
    out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1
    out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2
    ```
    
<br>

### **AutoGrad**
PyTorch의 핵심. 자동 미분   
- `.backward()` 함수 사용   

<br>

### **nn.Module**
- nn.Sequential()
- nn.ModuleDict()
- Parameter(torch.ones((out_, in_))) : 1로 초기화
- Buffer