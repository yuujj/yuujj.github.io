---
layout : post
title:  "[부스트캠프 AI] Day8"
excerpt: "2주차 학습기록"

categories: ["Boostcamp AI", "Python", "PyTorch"]
tags: ["Boostcamp AI"]

published: true
toc: true
toc_sticky: true
 
date: 2022-01-26
last_modified_at: 2022-01-30
use_math: true
---

# <b>[PyTorch]</b>
---

## **모델 불러오기**

### **model.save()**

- 모델 형태(architecture)와 파라미터 저장
- 모델 학습 중간 과정의 저장을 통해 최선의 결과 모델 선택
- state_dict() : 모델의 파라미터 표시

```python
# https://tutorials.pytorch.kr/beginner/saving_loading_models.html
## 모델의 파라미터 저장하여 파라미터 load
# 모델과 파라미터 저장 
for params in model.state_dict():  
	print(params, "\t", model.state_dict()[params].size()) 

torch.save(model.state_dict(), PATH+'model.pt')  # ordered dict 형태

# 같은 모델 형태에서 파라미터만 load 하기
n_model = TheModelClass()
n_model.load_state_dict(torch.load(PATH+'model.pt'))

## 모델의 architecture와 함께 저장 & load
torch.save(model, PATH+'model.pt')
model = torch.load(PATH+'model.pt')
```

<br>

### **checkpoints**

- 학습의 중간 결과를 저장 → **최선의 결과 선택**
- earlystopping 사용시 이전 학습 결과물을 저장
- loss, metric 값을 지속적으로 확인 저장 (epoch, loss, metric)

```python
# https://tutorials.pytorch.kr/beginner/saving_loading_models.html
torch.save({'epoch' : epoch,
            'model_state_dict' : model.state_dict(),
            'optimizer_state_dict' : optimizer.sate_dict(),
            'loss' : loss}, "file_name_.pt")

checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
```

<br>

### **pretrained model Transfer learning**

다른 데이터셋으로 만든 모델을 현재 데이터에 적용

- 대용량 데이터셋으로 학습한 모델의 성능이 좋음
- 현재의 DL에서는 가장 일반적인 학습 기법
- backbone architecture가 잘 학습된 모델에서 일부분만 변경하여 학습 수행
- 모델의 일부분을 frozen 시킴
    : 모델을 가져와서 쓰되, 특정 위치까지만 쓰고 이후부터는 새로 학습시킴
    
    → 기준 이전 : 파라미터 변경 X, 기준 이후 : 파라미터 업데이트
    

```python
#https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html
## vgg16 모델을 가져오기
model = models.vgg16(pretrained=True)

## vgg19 모델 마지막 레이어 제외하고 frozen
class Mymodel(nn.Module):
		def __init__(self):
			...
			self.vgg19 = models.vgg19(pretrained=True)
			self.linear_layers = nn.Linear(input, output)

		def forward(self, x):
			x = self.vgg19(x)
			return self.linear_layers(x)   # vgg 이후 마지막

for param in my_model.parameters():
	param.requires_grad = False        # 마지막 레이어 제외 Frozen
for param in my_model.linear_layers.parameters():
	param.requires_grad = True         # 마지막 레이어는 학습(업데이트)
```

<br>

---
## **Monitoring tools for PyTorch**

### **Tensorboard**

TensorFlow의 프로젝트로 만들어진 시각화 도구 → PyTorch도 연결 가능

- 학습 그래프, metric, 학습 결과의 시각화를 지원
- DL 시각화
- 구성
    - scalar : 상수 값의 연속(epoch)
    - graph : 모델의 computational graph
    - histogram : weight 등 값의 분포
    - Image : 예측-실제 값 비교 표시
    - mesh : 3d 형태의 데이터 표현

<br>

### **weight & biases(wandb)**

머신러닝 실험 원활히 지원 위한 도구

- 협업, code versioning, 실험 결과 기록 등 제공
- MLOps

<br>

---

## **Dataset & DataLoader**

### **class Dataset(Dataset)**

```python
class DatasetA(Dataset):
	def __init__(self):
		A = load_A()
		self.X = torch.tensor(A.data)
		self.y = torch.tensor(A.target)
		self.feature_names = A.feature_names

	def __len__(self):
		return len(self.X)

	def __getitem__(self, idx):
		return torch.tensor(self.X[idx]), torch.tensor(self.y[idx])

```

<br>

### **collate_fn**

하나의 batch에서 가장 길이가 긴 sample 기준으로 길이 맞추기

```python
def my_collate_fn(samples):
	inputs = [sample['X'] for sample in samples]
	outputs = [sample['y'] for sample in samples]
	coll_in = torch.nn.utils.rnn.pad_sequence(collate_X, batch_first=True)
	
	return {'X': torch.stack(list(coll_in)),  # 또는 coll_in.couguous(),
					'y': torch.stack(collate_Y)}
```