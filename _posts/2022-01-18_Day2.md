---
title:  "[부스트캠프 AI] Day2"
excerpt: "1주차 학습기록"

categories:
  - [Boostcamp AI]
tags:
  - [Boostcamp AI]

toc: true
toc_sticky: true
 
date: 2022-01-18
last_modified_at: 2022-01-23
---


# **[Python]**
## **2-2. Function and Console I/O**
- GUI, CLI
- console in/out : input(), print()
- formating : %, str, padding, naming, f-string
<br></br>


## **2-3. Conditionals and Loops**
### **반복문**
- 변수명 : i, j, k (x, y, z와 같은 관례)
- 0부터 시작 (2진수가 0부터 시작)
- 무한 loop : CPU, 메모리 등 컴퓨터 리소스 과다하게 점유하니 무한 loop에 빠지지 않도록 주의
<br></br>

## **2-4. String and adcanced function concept**
- 함수 호출 방식 : Call by Value(값만 넘김), Call by Reference(메모리 주소 넘김)
- 변수의 범위 : 지역변수(local variable) - 함수내 / 전역변수(Global variable) - 프로그램 전체
- docstring : 함수에 대한 상세스펙을 사전에 작성 → 사용자 이행도 UP
     ⇒ VS code ‘docstring generator’
    
- 함수 개발 가이드라인
    - 공통적으로 사용되는 코드, 복잡한 수식, 복잡한 조건 ⇒ 함수화
    - 다른 사람이 쉽게 사용할 수 있도록 작성
    - 규칙 : 코딩 컨벤션(구글 파이썬 컨벤션 찾아보기)
        - 체크 : ‘flacke8’모듈 - 확인 가능 / ‘black’ 모듈 - 자동 변경 가능

<br></br>
<br></br>
# **[AI Math]**
## **3. 경사하강법 - 순한맛**
### **미분(differentiation)**  `sympy.diff`
&nbsp;: 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구
- 최적화에서 가장 많이 사용
- 변화율(기울기)의 극한(limit)으로 정리   
&nbsp;
### **미분의 이해**
&nbsp;: 미분은 함수 f의 주어진 점 $(x, f(x))$에서의 ‘접선의 기울기’
- 경사상승법(gradient ascent) : 미분값 더하기. 함수 극대값의 위치 구함(목적함수 최대화)
- 경사하강법(gradient descent) : 미분값 빼기. 함수 극소값의 위치 구함(목적함수 최소화)
- 극값에 도달 시 멈춤 ⇒ 극값에서는 미분 값 0   
&nbsp;
### **경사하강법(gradient descent)**
- Input : gradient(미분 계산 함수), init(시작점), lr(학습률), eps(알고리즘 종료조건)
- Output : var
- 벡터가 입력인 다변수 함수 ⇒ 편미분(partial differentiation) 사용
    - 각 변수별 편미분 계산한 gradient vector 이용하여 경사하강/상승법에 사용 가능
    - 알고리즘은 그대로 적용, 절대값 대신 노름(norm)계산해서 종료조건 설정

<br></br>
## **4. 경사하강법 - 매운맛**
### **선형회귀**
- 선형회귀의 목적식($||y-X\beta||_2$)을 최소화하는 $\beta$ 찾는 것이 목적
- $\beta$ 최소화 : 목적식을 $\beta$로 미분 → 주어진 $\beta$에서 미분 값 빼줌 ⇒ 경사하강법
- L2-노름의 제곱 사용 : L2-노름을 최소화 하는 $\beta$ 찾기, L2-노름의 제곱 최소화하는 $\beta$ ⇒ 같은 결과 값   
&nbsp;
### **경사하강법 기반 선형 회귀 알고리즘**
- Input : X, y, lr(학습률)*, T(학습횟수)*    *중요한 hyperparameter
- Output : beta
- $\bigtriangledown_\beta||y-X\beta||^2_2$ 항 계산하여 $\beta$ 업데이트  ⇒ 무어펜로즈 역행렬 이용X 계수 구할 수 있음
- lr 큰 경우 : 불안정 / 작은경우 : 정답 찾지 못하고 끝   
&nbsp;
### **경사하강법, SGD**
- 경사하강법
&nbsp;: 미분 가능, 볼록한 함수 → 적절한 학습률, 학습횟수 선택시 수렴 보장
- 비선형회귀의 경우 목적식이 대부분 볼록함수가 아니어서 항상 보장 되지 않음   
   ⇒ 변형된 경사하강법 사용
- 확률적경사하강법(stochastic gradient descent, SGD    
    : 모든 데이터X, 한개 또는 일부(mini-batch) 활용하여 업데이트 
    - 볼록이 아닌 목적식 → SGD를 통해 회적화
    - 확률적으로 선택하므로 목적식 모양이 바뀌게 됨. 값은 다르겠지만 방향은 유사할 것으로 기대
    - 일부만 사용하므로 알고리즘의 효율성, 하드웨어적으로 머신러닝 학습에 더 효율적