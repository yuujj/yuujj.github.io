---
title:  "[부스트캠프 AI] "
excerpt: "1주차 학습기록_test"

categories:
  - BoostcampAI
tags:
  - [BoostcampAI]

toc: true
toc_sticky: true
 
date: 2022-01-20
last_modified_at: 2022-01-20
---
# <b>[AI Math 5]</b>

## <b>신경망을 수식으로 분해해보기</b>  
  ⇒ 비선형모델인 신경망(neural network)
- 행벡터 $O_i$는 데이터 $X_i$와 가중치 행렬 $W$사이의 행렬곱과 절편 $b$벡터의 합으로 표현
    
    <!-- ![스크린샷 2022-01-20 오전 1.23.37.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0d957a0e-8edf-4525-bedd-5ae64c56cfa6/스크린샷_2022-01-20_오전_1.23.37.png) -->
    
- $X_i$ : 데이터 , $W$: 가중치 행렬(다른 데이터 공간으로 보내줌), $b$ : y절편(각 행이 같음)
- 출력 차원 : n x p   ((n x d) x (d x p))
- 신경망 : 선형모델과 활성함수(activation fuction)를 합성한 함수  
<br></br>
## <b>softmax</b>
        
<!-- ![스크린샷 2022-01-20 오전 1.25.10.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/37b738f6-f3d9-4741-a713-55216381baa3/스크린샷_2022-01-20_오전_1.25.10.png) -->
        
- 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산
- 출력 벡터 o에 softmax 함수를 합성하면 확률벡터가 되므로 ‘특정 클래스 k에 속할 확률’로 해석 가능
- 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측
$softmax(o) = softmax(W_x+b)$
<br></br>
## <b>활성함수(activation function)</b>   
- 활성함수를 쓰지 않으면 딥러닝은 선형모델과 차이가 없음   
- sigmoid, tanh, ReLU (나중에 자료 추가)   
<br></br>
## <b>다층 퍼셉트론(MLP)</b>
   : 신경망이 여러층 합성된 함수
- MLP의 파라미터는 L개의 가중치 행렬 $W^1$~$W^n$과 $b^1$~$b^n$으로 이루어져 있음
- $l = 1, ..., L$까지 순차적인 신경망 계산 ⇒ 순전파 (학습X 주어진 데이터 → 출력으로 내뱉는 것)
- 층을 여러개 쌓는 이유?
    - 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 빠르게 줄어들어 효율적인 학습 가능
    - 층이 얇으면 필요한 뉴런의 숫자가 기하급수적으로 늘어나서 넓은 신경망이 되어야함
    - 층이 깊으면 복잡한 문제 풀 수 있지만 최적화 어려움
<br></br>   
## <b>딥러닝 학습 원리 : 역전파(backpropagation)</b>
- 딥러닝은 역전파 알고리즘을 이용하여 각 층에 사용된 파라미터 $\{W^{(l)}, b^{(l)}\}^L_{l=1}$를 학습
- 손실함수를 $l$라 했을 때 $dl/dw^{(l)}$ 정보 계산에 사용
- $l = L, ..., 1$ 순서로 연쇄법칙을 통해 그래디언트 벡터를 전달
- 미분 한번에 계산X, 순차적 계산 (윗층부터 역순으로)